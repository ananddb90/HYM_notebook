{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2021 Robert Bosch GmbH\n",
    "\n",
    "All rights reserved.\n",
    "\n",
    "This source code is licensed under the MIT license found in the LICENSE file in the root directory of this source tree.\n",
    "\n",
    "@author: [Barbara Rakitsch](mailto:barbara.rakitsch@de.bosch.com)\n",
    "\n",
    "\n",
    "# Hybrid Modeling Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/img/schema.png\" style=\"width: 380px; padding: 0px; padding-top:0px\" img align=\"right\">\n",
    "\n",
    "Hybrid Modeling is a modeling technique in which a physics-based model is combined with a data-based approach. By combining the best of both worlds, we trade off the benefit of prior knowledge when training data is scarce with the flexibility of a data driven approaches when training data is abundant.\n",
    "\n",
    "It is in particular suited for applications in which an exact physics-based model cannot be obtained as either the underlying physics are not completely understood or the runtime is expensive. In these cases, one can restore to a simpler approximation to the model and equip it with an additional data-based component.\n",
    "\n",
    "Compared to pure data driven approaches, hybrid methods have the benefits of improved extrapolation behavior since they revert back to the physcics-based model when training data is scarce. In addition, they require less training data since they do not start learning from scratch but need only to learn what the physics-based model does not know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this Notebook\n",
    "In this notebook we will cover\n",
    "\n",
    "- residual modeling for hybrid tasks\n",
    "- nested optimization for joint parameter learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many applications require to learn complex relationships from data. In regression problems, the task is to learn a function $f:\\mathbb{R}^D \\rightarrow \\mathbb{R}$ that maps an input $x \\in \\mathbb{R}^D$ to a noisy observation $y \\in \\mathbb{R}$. \n",
    "\n",
    "While there exists a large number of different regression approaches, one can categorize them broadly into a) data driven, b) physics-based and c) hybrid approaches. Data driven approaches extract information from training data to generalize to new (unseen) data by either learning a larger number of parameters or by following a non-parametric approach. In non-parametric approaches, the number of parameters is growing with the number of data points. In contrast, physics-based models assume a model from first principles in which only few parameters need to be identified by the use of training data. The focus of this tutorial are hybrid approaches that combine data driven and physics-based approaches. To make the dependency of $f$ on  the parameters $\\theta$ more explicit, we will write in the following $f(x; \\theta)$.\n",
    "\n",
    "Hybrid methods are a compelling alternative to pure data- or physics-based approaches as they offer the benefits of both worlds: In case that the training data is scarce, one can exploit the prior knowledge of the physics-based model to obtain accurate predictions. In case that training data is abundant, the data driven approach can capture details of the system that are absent in the physics-based model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual learning and nested optimization\n",
    "In the following, we assume that the function $f(x; \\theta)$ can be expressed as a sum of the physics-based and the data-driven component $f(x; \\theta) = f_p( x; \\mu) + f_d(x; \\psi)$, where $f_p: \\mathbb{R}^D \\rightarrow \\mathbb{R}$ describes the physics-based and $f_d: \\mathbb{R}^D \\rightarrow \\mathbb{R}$ the data-based component. \n",
    "The physics-based and the data-based component are parameterized by two distinct sets of parameters $\\mu, \\psi$ with $\\theta = \\{\\mu, \\psi\\}$ and $\\mu \\cap \\psi = \\emptyset$.\n",
    "\n",
    "In general, we cannot assume that the physics-based component $f_p(x; \\mu)$ and the data driven component $f_d(x; \\psi)$ are available in the same programming environment since many simulators are coded in C or Simulink, while Python and Matlab are often the language of choice in the machine learning community.\n",
    "Identifying the parameters $\\mu$ of the physics-based component is therefore difficult since we cannot expect to have access to the gradients of $\\mu$ and the optimal parameter setting depends on the parameters $\\psi$ of the data-based component.\n",
    "We circumvent the former by using a gradient-free optimizer and the latter by employing a nested optimization strategy as outlined below:\n",
    "\n",
    "<br><font color='green'>**while**</font> $\\mu$ <font color='green'>**not converged**</font> \n",
    "<br>&emsp; Update $\\mu$ using gradient-free optimization.\n",
    "<br>&emsp; Compute the residuals $r_n = y_n - f_p(x_n; \\mu)$.\n",
    "<br>&emsp; Update $\\psi$ on $(x_n, r_n)_{n=1}^N$ using gradient-based optimization.\n",
    "<br><font color='green'>**end**</font>\n",
    "<br>\n",
    "\n",
    "In the outer loop, we optimize the parameters of the physics-based component $\\mu$, while we optimize the parameters $\\psi$ of the data-based component in the inner loop.\n",
    "In each iteration, we first update the parameters $\\mu$ of the physics-based model using gradient-free optimization. We then use the predictions of the physics-based model to\n",
    "compute the residuals $r_n = y_n -  f_p(x_n; \\mu)$. Afterwards, we fit the parameters of the data driven approach $\\psi$ to the residual dataset $(x_n, r_n)_{n=1}^N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/img/gp_example.png\" style=\"width: 240px; padding: 0px; padding-top:0px\" img align=\"right\">\n",
    "\n",
    "## Data-Based Modeling with Gaussian Processes\n",
    "We employ Gaussian Processes for leaning the function $f_d$. Gaussian Processes are a powerful non-parameteric method for learning complex functions. They belong to the family of probabilistic methods that predict not only point estimates but equip each prediction with uncertainty estimates. For a more careful treatment of Gaussian Processes, we refer the reader to our [GP tutorial](https://github.com/boschresearch/GP_tutorial) that is also provided on the Bosch github repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/img/bo_example.png\" style=\"width: 240px; padding: 0px; padding-top:30px\" img align=\"right\">\n",
    "\n",
    "## Parameter Identification with Bayesian Optimization\n",
    "We apply Bayesian Optimization for learning the parameters $\\psi$ without gradient information. Bayesian Optimization is in particular suited for optimization problems in which the evaluation of the objective is time-consuming since it requires only few evaluations. Note that this is also the case in our scenario since we have to learn the data-based component $f_d$ in each iteration from scratch and need to evaluate the physical component $f_p$ for each data point $x_n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installed Packages\n",
    "We will use the libary ``GPy`` for the Gaussian Processes Regression and ``GPyOpt`` for Bayesian Optimization. ``Numpy``, ``Scipy`` and ``Matplotlib`` are needed for mathematical operations and visualization/plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "from src.helpers import rmse, create_plot \n",
    "\n",
    "import GPy\n",
    "import GPyOpt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = np.loadtxt('input/time.csv')[:,np.newaxis] # load time\n",
    "y = np.loadtxt('input/y.csv')[:,np.newaxis] # load outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create training and test data\n",
    "N = t.shape[0]\n",
    "idx_test1 = np.zeros(N, dtype = np.bool) # in-sample test points\n",
    "idx_test1[np.sort(np.random.permutation(N)[:100])] = 1 \n",
    "idx_test1[np.logical_and(5<t[:,0], t[:,0]<15)]= 0\n",
    "idx_test2 = np.zeros(N, dtype = np.bool) #out-of-sample test points\n",
    "idx_test2[np.logical_and(5<t[:,0], t[:,0]<15)]= 1 \n",
    "idx_train = np.ones(N, dtype = np.bool) # delete test from training points\n",
    "idx_train[idx_test1 | idx_test2] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's plot the data\n",
    "fig = plt.figure(figsize=(12,3))\n",
    "plt.plot(t[idx_train], y[idx_train], '.', label='train') # training data\n",
    "plt.plot(t[idx_test1], y[idx_test1], '.', label='test1') # in-sample test points\n",
    "plt.plot(t[idx_test2], y[idx_test2], '.', label='test2') # out-sample test points\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "In the following, we will contrast three learning approaches:\n",
    "a data driven approach using Gaussian Processes,\n",
    "a physics-based approach assuming that the dynamics follow the Van der Pol equations, and\n",
    "a hybrid approach combining the former two.\n",
    "We start with the pure data driven approach using a Gaussian Process model. \n",
    "\n",
    "In this toy example, all models use time (denoted by $t$) as inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GP:\n",
    "    \"\"\"\n",
    "    Gaussian Process model\n",
    "    \"\"\"\n",
    "    def train(self, t, y, idx_train):\n",
    "        \"\"\"\n",
    "        training the GP model,\n",
    "        \n",
    "        y ~ GP (m, K(t, t'))\n",
    "        \n",
    "        where m is an constant and K is the rbf kernel. Using a constant mean enables us to learn an offset, \n",
    "        the RBF kernel is universal, i.e. it can approximate any desired function. \n",
    "        We optimize the hyperparameters of the kernel and the variance of the noise by maximizing the \n",
    "        log marginal likelihood of the training data (x, y). \n",
    "        \n",
    "        Args:\n",
    "            t           :    time points [Nx1]\n",
    "            y           :    outputs [Nx1]\n",
    "            idx_train   :    training indices [N_train]\n",
    "        \"\"\"\n",
    "        k = GPy.kern.RBF(1)\n",
    "        mf = GPy.mappings.Linear(1,1)\n",
    "        self.gpr = GPy.models.gp_regression.GPRegression(t[idx_train], y[idx_train], kernel=k, mean_function=mf)\n",
    "        self.gpr.optimize()\n",
    "        \n",
    "        \n",
    "    def predict(self, t):\n",
    "        \"\"\"\n",
    "        returns mean and variance of GP predictions.\n",
    "        \n",
    "        Args:\n",
    "            t        :    time points [Nx1]\n",
    "            \n",
    "        Returns:\n",
    "            (arr,arr)  :    mean and variance predictions of size [Nx1]\n",
    "        \"\"\"\n",
    "        return self.gpr.predict(t)\n",
    "    \n",
    "    \n",
    "    def nll(self):\n",
    "        \"\"\"\n",
    "        returns negative log likelood of the trained model\n",
    "        \"\"\"\n",
    "        return - self.gpr.log_likelihood()\n",
    "    \n",
    "    \n",
    "model = GP()\n",
    "model.train(t, y, idx_train)\n",
    "ygp_mean, ygp_var = model.predict(t)   \n",
    "print('... RMSE: %.2f'%rmse(y[idx_test1], ygp_mean[idx_test1]))\n",
    "print('... RMSE: %.2f'%rmse(y[idx_test2], ygp_var[idx_test2]))\n",
    "create_plot(t, y, idx_train, idx_test1, idx_test2, ygp_mean, ygp_var, title='GP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the GP performs a good job when training data is abundant. However, if training data is scarce (between 5 and 15 time units), the predictions revert to the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Van der Pol Oscillator\n",
    "\n",
    "Our output signal roughly follows the Van der Pol Equations\n",
    "\n",
    "$$\n",
    "\\frac{d^2z_t}{dt^2} - \\mu (1 - z_t^2) \\frac{dz_t}{dt} + z_t = 0,\n",
    "$$\n",
    "\n",
    "where the noise-free signal at time point $t$ is denoted by $z_t \\in \\mathbb{R}$, and $\\mu \\in \\mathbb{R}^+$ is a parameter describing the strength of the non-linear damping. \n",
    "We can convert this equation into its state-space formulation with state $h_t=(s_t, v_t)=(z_t, \\frac{dz_t}{d_t})$, and $\\frac{dh_t}{dt} = f_{\\text{ODE}} (h_t; \\mu)$ is defined by the differential $ f_{\\text{ODE}}:\\mathbb{R}^2 \\rightarrow \\mathbb{R}^2:(s_t, v_t) \\rightarrow (v_t,\\ \\mu (1 - s_t^2) v_t - s_t)$. Given the initial latent state, $h_0$, we can simulate the system up to time point $t$ as follows\n",
    "$$\n",
    "h_t = h_0 + \\int_{0}^t f_{\\text{ODE}}(h_\\tau; \\mu) d\\tau.\n",
    "$$\n",
    "Furthermore, we can cast this approach into our framework by restricting the input $x$ to a one-dimensional scalar that can be interpreted as time $t$. The physics-based model is then given by $f_p(t; \\mu) = f_e \\left(h_0 + \\int_{0}^t f_{\\text{ODE}}(h_\\tau; \\mu) d\\tau \\right)$, where $f_e:\\mathbb{R}^2\\rightarrow\\mathbb{R}:(s_t, v_t) \\rightarrow s_t$ is the emission function.\n",
    "\n",
    "For solving the differential equation, we use the scipy method ``ivp_solve`` which uses the Runge-Kutta method of order 5(4)\n",
    "as numerical solver.\n",
    "The parameter $\\mu$ is not known in advance and we want to estimate it by matching the training data to the ODE system. Since we cannot in general assume that we have access to the gradients of $\\mu$, we use in the following Bayesian Optimization for estimating the parameter $\\mu$. Assuming Gaussian noise, we can minimize the mean squared error to find the optimal parameter setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VDP:\n",
    "    \"\"\"\n",
    "    Van der Pol Oscillator\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.domain = [{'name': 'mu', 'type': 'continuous', 'domain': (1,10)}] \n",
    "     \n",
    "    def setParam(self, mu):\n",
    "        \"\"\"\n",
    "        setting the damping parameter mu\n",
    "        \n",
    "        Args:\n",
    "            mu    :    float\n",
    "        \"\"\"\n",
    "        self.mu = mu \n",
    "        \n",
    "    def eval(self, t, h):\n",
    "        \"\"\"\n",
    "        evaluates differential equation\n",
    "        \n",
    "        Args:\n",
    "            t    :    time point (float)\n",
    "            h    :    two-dimensional state\n",
    "        \"\"\"\n",
    "        s, v = h\n",
    "        ds_dt = v \n",
    "        dv_dt = self.mu*(1 - s**2)*v - s\n",
    "        return [ds_dt, dv_dt]\n",
    "    \n",
    "    \n",
    "    def train(self, t, y, idx_train):\n",
    "        \"\"\"\n",
    "        trains VDP model.\n",
    "        \n",
    "        We optimize the damping factor mu by minimizing the mean squared error between the true \n",
    "        and the predicted observations using Bayesian Optimization.\n",
    "        \n",
    "        Args:\n",
    "            t           :    time points [Nx1]\n",
    "            y           :    outputs [Nx1]\n",
    "            idx_train   :    training indices [N_train]\n",
    "        \"\"\"\n",
    "        \n",
    "        def fun(mu):\n",
    "            \"\"\"\n",
    "            evaluates the mean squared error between the true and the predicted outcome\n",
    "            for the damping factor mu.\n",
    "            \n",
    "            Args:\n",
    "                mu    :    damping parameter\n",
    "            \"\"\"\n",
    "            self.setParam(mu)\n",
    "            yph = self.predict(t)\n",
    "            return rmse(yph[idx_train], y[idx_train])**2\n",
    "        \n",
    "        myBopt = GPyOpt.methods.BayesianOptimization(f=fun, domain=self.domain)\n",
    "        myBopt.run_optimization(max_iter=15)\n",
    "        mu_opt = myBopt.x_opt[0]\n",
    "        self.setParam(mu_opt)\n",
    "    \n",
    "    def predict(self,t):\n",
    "        \"\"\"\n",
    "        returns (deterministic) predictions of the physical model.\n",
    "        \n",
    "        Args:\n",
    "            t        :    time points [Nx1]\n",
    "            \n",
    "        Returns:\n",
    "            (arr)    :    predictions of size [Nx1]\n",
    "        \"\"\"\n",
    "        (a, b) = (t.min(), t.max()) # time span \n",
    "        h0 = [1,0] # initial state\n",
    "        sol = solve_ivp(self.eval, [a,b], h0, t_eval=t[:,0]) # numerical integration \n",
    "        return sol.y[0][:,np.newaxis] # emission function\n",
    "\n",
    "\n",
    "model = VDP()\n",
    "model.train(t, y, idx_train)\n",
    "yph = model.predict(t)\n",
    "print('... RMSE: %.2f'%rmse(y[idx_test1], yph[idx_test1]))\n",
    "print('... RMSE: %.2f'%rmse(y[idx_test2], yph[idx_test2]))\n",
    "create_plot(t, y, idx_train, idx_test1, idx_test2, yph, title='Van der Pol Oscillator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the Van der Pol Oscillator performs better when training data is absent (between 5 and 15 time units), while its performance detoriates when training data is abundant. In addition, we do not obtain uncertainty estimates when compared to the GP approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Approach\n",
    "In the following, we combine the Van der Pol Oscillator with the Gaussian Process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hybrid:\n",
    "    \"\"\"\n",
    "    Hybrid model: f(t) = f_d(t) + f_p(t).\n",
    "    \n",
    "    We assume in the following that the physics-based component f_p(t) is deterministic, \n",
    "    while the data-driven component is probabilistic\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_data, model_phys):\n",
    "        self.model_data = model_data\n",
    "        self.model_phys = model_phys\n",
    "        \n",
    "    def train(self, t, y, idx_train):\n",
    "        \"\"\"\n",
    "        trains hybrid model.\n",
    "        \n",
    "        We optimize the parameters of the physical model by minimizing the negative log marginal\n",
    "        likelihood of the hybrid model.\n",
    "             \n",
    "        Args:\n",
    "            t           :    time points [Nx1]\n",
    "            y           :    outputs [Nx1]\n",
    "            idx_train   :    training indices [N_train]\n",
    "        \"\"\"\n",
    "        \n",
    "        def fun(params):\n",
    "            \"\"\"\n",
    "            evaluates the negative log likelihood of the hybrid model.\n",
    "            \n",
    "            Args:\n",
    "                params   :    parameter of the physics based model\n",
    "            \"\"\"\n",
    "            # evaluate physical model\n",
    "            self.model_phys.setParam(params)\n",
    "            y_ph = self.model_phys.predict(t)\n",
    "            # compute residuals\n",
    "            y_res = y - y_ph\n",
    "            # train data driven model on residuals\n",
    "            self.model_data.train(t, y_res, idx_train)   \n",
    "            # evaluate negative log likelihood\n",
    "            return self.model_data.nll()\n",
    "            \n",
    "        # optimizes the parameters of the physics model using nested Bayesian optimization\n",
    "        myBopt = GPyOpt.methods.BayesianOptimization(f=fun, domain=self.model_phys.domain)\n",
    "        myBopt.run_optimization(max_iter=15)\n",
    "        params_opt = myBopt.x_opt[0]\n",
    "        \n",
    "        # train data driven model with optimized physical parameters\n",
    "        self.model_phys.setParam(params_opt)\n",
    "        y_phys = self.model_phys.predict(t)\n",
    "        y_res = y - y_phys\n",
    "        self.model_data.train(t, y_res, idx_train)\n",
    "    \n",
    "    \n",
    "    def predict(self, t):\n",
    "        \"\"\"\n",
    "        returns mean and variance predictions of the hybrid model.\n",
    "        \n",
    "        Args:\n",
    "            t        :    time points [Nx1]\n",
    "            \n",
    "        Returns:\n",
    "            (arr,arr)  :    mean and variance predictions of size [Nx1]\n",
    "        \"\"\"\n",
    "        y_phys = self.model_phys.predict(t)\n",
    "        ygp_mean, ygp_var = self.model_data.predict(t)\n",
    "        return y_phys + ygp_mean, ygp_var\n",
    "\n",
    "vdp = VDP()\n",
    "gp = GP()\n",
    "model = Hybrid(gp, vdp)\n",
    "model.train(t,y, idx_train)\n",
    "yhybrid_mean, yhybrid_var = model.predict(t)\n",
    "\n",
    "print('... RMSE: %.2f'%rmse(y[idx_test1], yhybrid_mean[idx_test1]))\n",
    "print('... RMSE: %.2f'%rmse(y[idx_test2], yhybrid_mean[idx_test2]))\n",
    "\n",
    "create_plot(t, y, idx_train, idx_test1, idx_test2, yhybrid_mean, yhybrid_var, title='Hybrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint optimization over the model and GP parameters is highly complex and non-convex. As a consequence, we observed that the optimization gets sometimes stuck in a local optimum. If this happens to you, it is best to re-run the optimization. The global optimum achieves superior performances compared to the baselines and our hybrid model combines the best of both worlds:\n",
    "\n",
    "- The predictions are equipped with uncertainty estimates.\n",
    "- When training data is absent (between 5 and 15 time units), the physics-based model takes over.\n",
    "- When training data is abundant, the GP is used to improve the predictions of the physics-based model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of the current approach and further research directions\n",
    "\n",
    "We want to note that our notebook is mostly designed to highlight the strength of hybrid modeling on a toy example. To get this approach working on a real-world example, one might need to extend the method in the following directions:\n",
    "\n",
    "- In our experiments, we considered that the initial state $h_0$ is known a-priori which might not hold in practice. Instead, one can consider to handle the initial state $h_0$ in a Bayesian manner, by placing a prior distribution over the initial state $h_0 \\sim p(h_0)$, and treat it like a latent variable during inference.\n",
    "- The current implementation allows for tuning the damping factor $\\mu$ only. Applications to real-world data might need to consider input and output transformations such that the data matches the physics-based equations. While in some applications simple affine transformations might be sufficient, one can also consider neural networks to learn an embedding from the potentially high-dimensional observed space to the latent space [1].\n",
    "- We used a Gaussian process with time as input as the data driven component which can be reformulated as a linear Gaussian state-space model [2]. Using the state-space model formulation allows one to considerably reduce the runtime which is needed for large-scale applications. However, it is important to note that this model class is restricted to linear dynamics and one might need to consider more powerful approaches based on deep neural networks or on Gaussian processes with more complicated features.\n",
    "- Our bilevel optimization scheme requires to learn the data-based component in each iteration from scratch which is time-consuming and can become prohibitive for large datasets.\n",
    "\n",
    "\n",
    "### References:\n",
    "\n",
    "[1] Yildiz, Cagatay, Markus Heinonen, and Harri Lahdesmaki. \"ODE2VAE: Deep generative second order odes with bayesian neural networks.\" Advances in Neural Information Processing Systems. 2019.\n",
    "\n",
    "[2] Hartikainen, Jouni, and Simo Särkkä. \"Kalman filtering and smoothing solutions to temporal Gaussian process regression models.\" 2010 IEEE international workshop on machine learning for signal processing. IEEE, 2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
